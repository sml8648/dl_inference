# dl_inference

## ê°œìš”
* FastAPIë¥¼ ì´ìš©í•˜ì—¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„œë¹™ apië¥¼ ìš”ì²­í•  ìˆ˜ ìˆëŠ” ì„œë²„ êµ¬ì¶•
* Dockerfileì„ ì´ìš©í•´ ë¹Œë“œí•˜ì—¬ ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì‹œí‚¬ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ êµ¬ì¶•

## ì„¸ë¶€ êµ¬í˜„ ì‚¬í•­
* í‘œì¤€ ì„œë¹™ í”„ë¡œí† ì½œì¸ v2 inference í”„ë¡œí† ì½œì—ì„œ HTTP í”„ë¡œí† ì½œë¡œ êµ¬í˜„
  - **Health** : GET v2/health/live GET v2/health/ready GET v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/ready
  - **Server Metadata** : GET v2
  - **Model Metadata** : GET v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]
  - **Inference** : POST v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/infer
  - ref : https://kserve.github.io/website/0.8/modelserving/inference_api/#httprest
  
 * TorchServeë¥¼ ì´ìš©í•˜ì—¬ ê³ ì„±ëŠ¥ ì¸í¼ëŸ°ìŠ¤ ì„œë²„ë¡œ ì¸í¼ëŸ°ìŠ¤ êµ¬ì„±
   - huggingfaceì˜ Transformer ëª¨ë¸ì„ ì„œë¹™(SequenceClassification, TokenClassification)
   - Naver cloudì˜ ê³ ì„±ëŠ¥ cpu ì„œë²„ë¥¼ í™œìš©(GPU ê°€ìš© ë¶ˆê°€)
   - ref : https://github.com/pytorch/serve/blob/master/README.md#serve-a-model
   
 * Github actionì„ ì´ìš©í•˜ì—¬ CI êµ¬ì¶•
 * Dockerfileì„ ì´ìš©í•˜ì—¬ msa í˜•íƒœë¡œ êµ¬ì¶•

## Directory structure
```
dl_inderence
â”œâ”€â”€ğŸ“logs
â”œâ”€â”€ğŸ“model_stroe -> torchserverë¥¼ ìœ„í•œ .mar file ì €ì¥
â”œâ”€â”€ğŸ“artifacts -> í›„ì²˜ë¦¬ë¥¼ ìœ„í•œ .json íŒŒì¼ ì €ì¥
â”œâ”€â”€ğŸ“Transformer_model -> .mar fileì„ ë§Œë“¤ê¸° ìœ„í•œ transformer ëª¨ë¸ ì €ì¥ê³µê°„
â”œâ”€â”€ğŸ“configs -> Transformer ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ ë°›ê¸°ìœ„í•œ config íŒŒì¼
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Transformer_download.py
â”œâ”€â”€ Transformer_handler_generalized.py
â”œâ”€â”€ main.py # fastapi server
â””â”€â”€ README.md
```
 
## ì„¤ì¹˜ ëª…ë ¹ì–´
```
git clone https://github.com/sml8648/dl_inference.git
cd dl_inference
docker build -t pytorch/torchserve
docker run --rm -it -p 8000:8000 8080:8080 -p 8081:8081 -p 8082:8082 -p 7070:7070 -p 7071:7071 pytorch/torchserve:latest
```

